<!-- Autogenerated by mlir-tblgen; don't manually edit -->
### `-sdy-close-shardings`

_Closes tensor shardings and drops replicated axes._


### `-sdy-drop-sharding-rules`

_Drops `OpShardingRuleAttr` from all registered ops._


### `-sdy-insert-explicit-reshards`

_Inserts explicit reshards to make all operations have compatible shardings._

A compatible sharding essentially means that the operation can accept the
sharded operands and produce a sharded result without requiring any reshard
communications (note that the operation might still require communication
such as all-reduce or halo-swaps).

After propagation, some operations may still have incompatible shardings.

Please note, when an axis (or sub-axis) is used to shard non-corresponding
dimensions (e.g. non-contracting dimensions in matmul) across multiple
tensors, or when an axis shards a dimension in one tensor but not the
corresponding dimension in the other tensor, it is said that the operation
has a sharding conflict. Hence, after this pass, the operations become
conflict-free.

This pass injects reshard operations explicitly so that, for each operation,
corresponding dimensions become sharded in the same way across all operands
and results, and every axis (or sub-axis) can only be used to shard a single
dimension type.

A clarifying example:

Input:
```mlir
mesh = <"x"=4, "y"=2>
%lhs : tensor<8x32xf32> {sdy.sharding=<@mesh, \[{"y"},{"x"}\]>}
%rhs : tensor<32x16xf32> {sdy.sharding=<@mesh, \[{"y"}, {"x"}\]>}
stablehlo.dot %lhs, %rhs {sdy.sharding_per_value=<[<@mesh, \[{"x"}, {}\]>]>}
  : (tensor<8x32xf32>, tensor<32x16xf32>) -> tensor<8x16xf32>
```

Output:
```mlir
sdy.mesh = <"x"=4, "y"=2>
%lhs : tensor<8x32xf32> {sdy.sharding=<@mesh, \[{"x"}, {"y"}\]>}
%rhs : tensor<32x16xf32> {sdy.sharding=<@mesh, \[{"y"}, {"x"}\]>}
%0 = sdy.reshard %rhs <@mesh, \[{"y"}, {}\]> : tensor<32x16xf32>
stablehlo.dot %lhs, %0 {sdy.sharding_per_value=<[<@mesh, \[{"x"}, {}\]>]>}
  : (tensor<8x32xf32>, tensor<32x16xf32>) -> tensor<8x16xf32>
```

In the example above, there is a conflict since `lhs` and `rhs` tensors
are both sharded on axis "x" on their non-contracting dimensions. Here,
`rhs` tensor is resharded, before the dot operation, explicitly to be
sharded only on its first dimension and on axis "x". This way, the dot
operation becomes compatible.
### `-sdy-remove-sharding-groups`

_Removes ShardingGroupOps after propagation._


### `-sdy-reshard-to-collectives`

_Converts ReshardOp into various Shardy collective ops._

Here we match reshard ops and rewrite them into various Shardy collective
 ops. After this pass, no reshard ops remain in the module. This pass assumes
 that xplicit reshards have already been inserted
 (`sdy-insert-explicit-reshards`).

 A clarifying example:

 Input:
 ```mlir
 mesh = <"x"=2, "y"=2, "z"=2>
 %0 : tensor<16x2xf32> {sdy.sharding<@mesh, \[{"x", "y", "z"}, {}\]>
 %1 = sdy.reshard %arg0 <@mesh, \[{"x"}, {}\]> : tensor<16x2xf32>
 ```

 Output:
 ```mlir
 mesh = <"x"=2, "y"=2, "z"=2>
 %0 : tensor<16x2xf32> {sdy.sharding<@mesh, \[{"x", "y", "z"}, {}\]>
 %1 = sdy.all_gather  \[{"y", "z"}, {}\] %arg0 out_sharding=<@mesh, \[{"x"}, {}\]> : tensor<16x2xf32>
 ```

 In the example above, the tensor `%0 : tensor<16x2xf32>` is sharded as
 `\[{"x", "y", "z"}, {}\]`. Then, there's a `reshard` op resharding it as
 `\[{"x"}, {}\]`. On the first axes, since the suffix `{"y", "z"}` is removed
 after the reshard, we infer that we have all-gathered `{"y", "z"}`. The
 second dimension is not changed.

### `-sdy-sharding-constraint-to-reshard`

_Converts ShardingConstraintOp into ReshardOp._


### `-sdy-sink-data-flow-edges`

_Sinks all `DataFlowEdgeOp` into their input._

Moves the sharding of each `DataFlowEdgeOp` to its input (the root target of
the edge), and replaces the op with its input.

TODO(tomnatan): consider moving the sharding to all targets that can have a
sharding attached.
### `-sdy-update-non-divisible-input-output-shardings`

_Makes FuncOp inputs/outputs evenly sharded, removing any need for padding due to non-divisible shardings._

Users of Shardy expect the function inputs/outputs to be evenly
divisible/shardable to avoid requiring padding their tensors. Propagation
may make inputs/outputs have non-divisible shardings, so this pass updates
them to the largest dimension sharding prefix of the original sharding that
is evenly sharded.
